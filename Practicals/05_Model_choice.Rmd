---
title: "Model assessment for georeferenced data in 2D"
subtitle: 'Day 2, Afternoon'
author: "Instructor: Sara Martino"
#date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: biblio.bib  
output:
  html_document:
    toc: yes
    toc_float: yes
    #code_download: yes
    toc_depth: 3
    number_sections: true

---




```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "~/INLA_course/Practicals/")

knitr::opts_chunk$set(echo = TRUE,  
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'),
               color = 'darkred',
               tooltip_message = 'Click to copy', tooltip_success = 'Done')
```

---

Start by loading useful libraries:
```{r}
library(ggplot2)
library(INLA)
library(inlabru)
library(viridis)
library(tidyverse)
library(patchwork)
```


# Data Simulation

We will now construct a 2D model with non-Gaussian observation noise, estimate three alternative models, and look at some model assessment tools. The model is as follows:
$$
y(s)\sim\text{T}_{\nu}(\eta(s), \tau);\ s\in\mathcal{D}
$$
Where $\text{T}_{\nu}(\cdot,\cdot)$ is a Student-t distribution with $\nu$ degrees of freedom, mean $\eta(s)$ and reparametrised so that the precision is $\tau$ for all the values of $\nu$.

Morevoer we have taht
$$
\eta(s) = \mu + u(s)
$$
where

* $\mu$ is a constant mean

* $u(s)$ is a Matern field over a 10X10 squared domain. 

First, we build a high resolution mesh for the true field, using low level INLA functions

```{r}
set.seed(12345L)
bnd <- spoly(data.frame(lon = c(0, 10, 10, 0), lat = c(0, 0, 10, 10)),
             crs=inla.CRS("longlat"))
mesh_fine <- inla.mesh.2d(boundary = bnd, max.edge = 0.2)

# Note: the priors here will not be used in estimation

matern_fine <-
  inla.spde2.pcmatern(mesh_fine, 
                      prior.sigma = c(1, 0.01), 
                      prior.range = c(1, 0.01))
# choose range and sd for the matern field
true_range <- 4
true_sigma <- 1

# Extract the Q matrix
true_Q <- inla.spde.precision(matern_fine, theta=log(c(true_range, true_sigma)))

# simulate from the matern field
true_field <- inla.qsample(1, true_Q, seed  = 1234L)[,1]

truth <- expand.grid(lon=seq(0, 10, length=100),
                     lat=seq(0, 10, length=100))
truth$field <- inla.mesh.project(mesh_fine,
                                 loc = as.matrix(truth),
                                 field = true_field)
coordinates(truth) <- c("lon", "lat")
truth <- as(truth, "SpatialPixelsDataFrame")

ggplot() + 
  gg(truth, mapping=aes_string("lon","lat",fill="field")) +
   coord_equal() +
              ggtitle("True field")+
  scale_fill_viridis()
```




 Extract observations from some random locations,
 and add  Student-T distributed observation noise:

```{r}
n <- 500
nu = 3
mydata <- data.frame(lon = runif(n, 0, 10), lat = runif(n, 0, 10))
mydata$observed <- inla.mesh.project(mesh_fine,
                                     loc = as.matrix(mydata),
                                     field = true_field) +
                   rt(n, df = nu)/sqrt(nu*(nu-2)) * 0.4
coordinates(mydata) <- c("lon", "lat")

ggplot() + gg(mydata, aes(color = observed)) +
  coord_equal() +
  scale_color_viridis() + ggtitle("Observed data")

```



# Fit the models

Construct a mesh covering the data:

```{r}
mesh <- inla.mesh.2d(boundary = bnd, 
                     cutoff = 2,
                     offset = c(1,2),
                     max.edge = c(0.7,1,5))
plot(mesh)
```


 Construct an SPDE model object for a Matern model:

```{r}
matern <-
  inla.spde2.pcmatern(mesh, 
                      prior.sigma = c(5, 0.01), 
                      prior.range = c(2, 0.01))
```


 Specify the model components:

```{r}
cmp <- observed ~ field(coordinates,
                        model = matern) +
                  Intercept(1)
```



We fit 3 different models to the dataset, with assessment scores activated.

* The fist model has  a Gaussian likelihood and only a common intercept

* The second  model has a Gaussian likelihood and a linear predictor consisting of a common mean and a Matern field.

* The third  model has a Student-t likelihood and a linear predictor consisting of a common mean and a Matern field.


```{r}
c.c <- list(cpo = TRUE,
            dic = TRUE,
            waic = TRUE,
            config = TRUE)

fit.zero <- bru( observed ~ Intercept(1), 
                data = mydata, 
                family="gaussian",
                options = list(control.compute = c.c))

fit.norm <- bru( cmp, 
                data = mydata, 
                family="gaussian",
                options = list(control.compute = c.c))

fit.T <- bru(cmp, 
             data = mydata, 
             family="T",
             options = list(control.compute = c.c))

```


<font size="3"> **Exercise  1:** </font>  Check the output of the three models. You can also compare the predictions at the observation points form the three models.


# Model check



## PIT values


 PIT are uniformly distributed under the true model. The usual plotting method for PIT is a histogram.
 
*  For models with too small predictive variance, the histogram tends to increase toward 0 and 1.
 
*  For models with too large predictive variance, the histogram tends to have peak in the middle.
 
*  For incorrectly skewed predictions, the histogram will tend to be skewed.
 
 Unfortunately, that doesn't necessarily imply that overfitting and oversmoothing can be detected.

 For the three estimated models we have :

```{r}
par(mfrow=c(1,3))
hist(fit.zero$cpo$pit, main="Simplistic model")
hist(fit.norm$cpo$pit, main="GRF model")
hist(fit.T$cpo$pit, main="GRF + T-model")
par(mfrow=c(1,1))
```



 The empirical cumulative distribution functions may be easier to analyse
 (a perfect model has the empirical CDF along the straight line):
```{r}
par(mfrow=c(1,3))
plot.ecdf(fit.zero$cpo$pit); abline(0, 1)
plot.ecdf(fit.norm$cpo$pit); abline(0, 1)
plot.ecdf(fit.T$cpo$pit); abline(0, 1)
```



 According to these plots, it's not clear if the model that matches the truth,
 the GRF + T-distribution model, isn't actually any better than the fully
 Gaussian model; even the simplistic model looks good. It sees the
 Gaussianness of the random field, and misses the non-Gaussian observation noise!
 

## CPO values

 Let's look at CPO instead. They are defined as:
 
 $$
\text{CPO}_i  = \pi(y_i^{\text{obs}}|y_j^\text{obs}, j\neq i)
$$

Hence, for each observation its CPO is the posterior probability of observing that observation when the model is fit using all data but $y_i$. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model to that observation and, perhaps, that it is an outlier.
 
 
 A measure that summarizes the CPO is:
 
 $$
 -\sum\log\text{CPO}_i
 $$
 with smaller values pointing to a better model fit.
 This measure   can be compared qualitatively with DIC and WAIC after multiplying
 with a factor 2:
 

```{r}
scores <- data.frame(
  cpo=c(zero = -sum(log(fit.zero$cpo$cpo)),
        norm = -sum(log(fit.norm$cpo$cpo)),
        T    = -sum(log(fit.T$cpo$cpo))),
  "cpo.2"=2*c(zero = -sum(log(fit.zero$cpo$cpo)),
              norm = -sum(log(fit.norm$cpo$cpo)),
              T    = -sum(log(fit.T$cpo$cpo))),
  dic=c(zero = fit.zero$waic$waic,
        norm = fit.norm$waic$waic,
        T    = fit.T$waic$waic),
  waic=c(zero = fit.zero$dic$dic,
         norm = fit.norm$dic$dic,
         T    = fit.T$dic$dic))
rownames(scores) <- c("simple_normal", "field_normal", "field_T")
scores
```



 We see that for the simplistic model that did not
 include any random field, all three scores are extremely close to each other.
 
 For the two random field models, the log-CPO-score appears to make a clearer
 distinction between the incorrect (Gaussian) and correct (Student-T) observation
 likelihood than DIC and WAIC does.

 Instead of comparing aggregated scores for different models, we can look at the
 distribution of individual log-CPO scores:
 
```{r}
data.frame(fit.zero = -log(fit.zero$cpo$cpo),
           fit.norm = -log(fit.norm$cpo$cpo),
           fit.T = -log(fit.T$cpo$cpo),
           i = 1:n) %>%
  pivot_longer(-i) %>%
  ggplot()+
  stat_ecdf(aes(value, group = name, color = name),geom = "step") +
  xlim(-0.1,3)
```


 The scores for the simplistic model (in black) are consistently large.
 The score distribution for the Gaussian likelihood model (in red) is
 shifted towards worse predictions (larger scores) than for the Student-T model.

 For model comparison, a potentially more useful approach is to consider the
 pairwise score differences for each observation: 

```{r}
cpo.zero.norm <- -log(fit.zero$cpo$cpo) + log(fit.norm$cpo$cpo)
cpo.zero.T <- -log(fit.zero$cpo$cpo) + log(fit.T$cpo$cpo)
cpo.norm.T <- -log(fit.norm$cpo$cpo) + log(fit.T$cpo$cpo)

```


 The empirical CDFs of the score differences can reveal if a model is
 consistently better or worse across all observation locations.
 Taking differences in the order of the aggregated model scores gives the following:

```{r}
data.frame("Simple.Normal" = cpo.zero.norm,
           "Simple.StudentT" = cpo.zero.T,
           "Normal.StudentT"= cpo.norm.T,
           i = 1:n) %>%
    pivot_longer(-i) %>%
ggplot() + 
  stat_ecdf(aes(value, group = name, color = name),geom = "step") +
  xlim(-2,4)  +
  geom_vline(data = data.frame(name = c("Simple.Normal", "Simple.StudentT", "Normal.StudentT"), 
                               val =c(mean(cpo.zero.norm), mean(cpo.zero.T), mean(cpo.norm.T))),
             aes(xintercept = val, color = name))+ facet_grid(name~.)
```



 We see that the Simple model is consistently worse than the other models.
 The difference between the Normal and Student T likelihood models is less consistent.
 
 Is there a pattern to which observations get improved predictions under the true model?
 Let do scatter plots of scores and score differences:

```{r}

p1 = data.frame(fitT = -log(fit.T$cpo$cpo),
           zero.T = cpo.zero.T) %>%
  ggplot() + geom_point(aes(fitT, zero.T)) +
  xlab("Student T logCPO scores") +
  ylab("diff simple T") 
p2 = data.frame(fitT = -log(fit.T$cpo$cpo),
           norm.T = cpo.norm.T) %>%
  ggplot() + geom_point(aes(fitT, norm.T)) +
  xlab("Student T logCPO scores") +
  ylab("diff normal T")
p1+p2+plot_layout(ncol = 1)

```



 The scatter plots are not so easy to interpret. There is a tendency
 for observations that are well predicted by the true, Student-T model (small scores)
 to have worse scores (positive differences in the plots) for the simpler models
 (the Simplistic and Normal likelihood models),
 and observations that are hard to predict under the true model (large scores)
 to have better scores (negative differences) in the Simplistic model. Hard to predict observations under the true model are also hard to predict under the normal model.

